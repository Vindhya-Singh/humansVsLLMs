{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nrclex import NRCLex\n",
    "from collections import Counter, defaultdict\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from json import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse for each LLM\n",
    "model = 'gemini'\n",
    "path = '/projects/humansVsLLMs/results/3-shot-generated-responses'\n",
    "df = pd.read_csv(f\"{path}/{model}_generated_responses.csv\")  # Replace with your file\n",
    "response_texts = df['Response'].dropna().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze goals of real-life leaders\n",
    "path = '/projects/humansVsLLMs/data'\n",
    "df = pd.read_csv(f\"{path}/goals_leader_with_demographics.csv\")  # Replace with your file\n",
    "response_texts = df['Leader_Action_Plans'].dropna().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gender wise comparison\n",
    "path = '/projects/humansVsLLMs/data'\n",
    "df = pd.read_csv(f\"{path}/data_leaders_with_demographics_semantics.csv\")  # Replace with your file\n",
    "df = df[~df['GenderIdentity'].isin(['Male', 'Female'])]\n",
    "response_texts_uniqueness = df['firstTaskGoal'].dropna().astype(str).to_list()\n",
    "print(len(response_texts_uniqueness))\n",
    "response_texts_belongingness = df['addFirstRelGoal'].dropna().astype(str).to_list()\n",
    "print(len(response_texts_belongingness))\n",
    "response_texts = response_texts_belongingness + response_texts_uniqueness\n",
    "print(len(response_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated containers\n",
    "all_affect_list = []\n",
    "all_affect_dict = defaultdict(int)\n",
    "all_raw_emotions = Counter()\n",
    "top_emotions_count = Counter()\n",
    "affect_freqs = defaultdict(float)\n",
    "\n",
    "# Process each response\n",
    "for response in response_texts:\n",
    "    text_object = NRCLex(response)\n",
    "   \n",
    "    # Affect list (adds all labels)\n",
    "    all_affect_list.extend(text_object.affect_list)\n",
    "    # print(text_object.affect_dict)\n",
    "    # print(text_object.raw_emotion_scores)\n",
    "    # print(text_object.top_emotions)\n",
    "\n",
    "    # Affect dictionary (sum over all keys)\n",
    "    # for k, v in text_object.affect_dict.items():\n",
    "        # all_affect_dict[k] += sum(v)\n",
    "\n",
    "    # Raw emotion scores\n",
    "    for k, v in text_object.raw_emotion_scores.items():\n",
    "        all_raw_emotions[k] += v\n",
    "\n",
    "    # Top emotions (count the most prominent labels)\n",
    "    for emotion in text_object.top_emotions:\n",
    "        top_emotions_count[emotion[0]] += 1\n",
    "\n",
    "    # Affect frequencies (normalized, so we'll average them later)\n",
    "    for k, v in text_object.affect_frequencies.items():\n",
    "        affect_freqs[k] += v\n",
    "\n",
    "# Normalize affect frequencies\n",
    "num_responses = len(response_texts)\n",
    "affect_freqs = {k: v / num_responses for k, v in affect_freqs.items()}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n Aggregated Affect List:\")\n",
    "print(all_affect_list[:10], \"...\")  # show sample\n",
    "\n",
    "print(\"\\n Aggregated Raw Emotion Scores:\")\n",
    "print(dict(all_raw_emotions))\n",
    "\n",
    "print(\"\\n Aggregated Top Emotions:\")\n",
    "print(dict(top_emotions_count))\n",
    "\n",
    "print(\"\\n Averaged Affect Frequencies:\")\n",
    "print(affect_freqs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

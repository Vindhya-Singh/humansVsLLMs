{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_model_bleu_scores(df):\n",
    "    \"\"\"\n",
    "    Calculate BLEU scores for each model based on the specified conditions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the evaluation data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with aggregated BLEU scores per model\n",
    "    \"\"\"\n",
    "    # Check required columns\n",
    "    required_columns = ['model_name', 'Dimension', 'firstTaskGoal', 'addTaskGoals', 'Selected_Text']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame is missing required column: {col}\")\n",
    "    \n",
    "    # Initialize results dictionary and smoothing function\n",
    "    results = {}\n",
    "    smoother = SmoothingFunction().method1\n",
    "    \n",
    "    # Get unique models\n",
    "    models = df['model_name'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        model_df = df[df['model_name'] == model]\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for _, row in model_df.iterrows():\n",
    "            # Determine reference text based on Dimension\n",
    "            if row['Dimension'] in ['Uniqueness', 'Appreciation']:\n",
    "                reference_text = row['firstTaskGoal'] if pd.notna(row['firstTaskGoal']) else row['addTaskGoals']\n",
    "            else:\n",
    "                reference_text = row['addFirstRelGoal'] if pd.notna(row['addFirstRelGoal']) else row['addRelGoals']  # Skip if Dimension doesn't match\n",
    "                \n",
    "            # Get candidate text\n",
    "            candidate_text = row['Selected_Text']\n",
    "            \n",
    "            # Skip if either text is missing\n",
    "            if pd.isna(reference_text) or pd.isna(candidate_text):\n",
    "                continue\n",
    "                \n",
    "            # Tokenize texts\n",
    "            try:\n",
    "                reference_tokens = [word_tokenize(str(reference_text).lower())]\n",
    "                candidate_tokens = word_tokenize(str(candidate_text).lower())\n",
    "                \n",
    "                # Calculate BLEU score\n",
    "                score = sentence_bleu(\n",
    "                    reference_tokens, \n",
    "                    candidate_tokens,\n",
    "                    smoothing_function=smoother\n",
    "                )\n",
    "                bleu_scores.append(score)\n",
    "            except:\n",
    "                continue  # Skip if tokenization fails\n",
    "        \n",
    "        # Store results for this model\n",
    "        if bleu_scores:\n",
    "            results[model] = {\n",
    "                'average_bleu': sum(bleu_scores) / len(bleu_scores),\n",
    "                'min_bleu': min(bleu_scores),\n",
    "                'max_bleu': max(bleu_scores),\n",
    "                'median_bleu': sorted(bleu_scores)[len(bleu_scores) // 2],\n",
    "                'num_comparisons': len(bleu_scores),\n",
    "                'all_scores': bleu_scores\n",
    "            }\n",
    "        else:\n",
    "            results[model] = {\n",
    "                'average_bleu': None,\n",
    "                'min_bleu': None,\n",
    "                'max_bleu': None,\n",
    "                'median_bleu': None,\n",
    "                'num_comparisons': 0,\n",
    "                'all_scores': []\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def read_and_process_file(file_path):\n",
    "    \"\"\"\n",
    "    Read and process the input file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file (CSV or Excel)\n",
    "        \n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Read file based on extension\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xls', '.xlsx')):\n",
    "        df = pd.read_excel(file_path, sheet_name='Final_Selection')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide CSV or Excel file.\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['model_name', 'Dimension', 'firstTaskGoal', 'addTaskGoals', 'addFirstRelGoal', 'addRelGoals', 'Selected_Text']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Input file is missing required column: {col}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    file_path = '/projects/humansVsLLMs/data/EMNLP-HUmanEvaluation.xlsx'  # Change to your file path\n",
    "    try:\n",
    "        # Read and process the file\n",
    "        df = read_and_process_file(file_path)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        results = calculate_model_bleu_scores(df)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"BLEU Score Aggregation by Model:\")\n",
    "        print(\"=\" * 50)\n",
    "        for model, scores in results.items():\n",
    "            print(f\"\\nModel: {model}\")\n",
    "            print(\"-\" * 30)\n",
    "            if scores['num_comparisons'] > 0:\n",
    "                print(f\"Average BLEU: {scores['average_bleu']:.4f}\")\n",
    "                print(f\"Minimum BLEU: {scores['min_bleu']:.4f}\")\n",
    "                print(f\"Maximum BLEU: {scores['max_bleu']:.4f}\")\n",
    "                print(f\"Median BLEU: {scores['median_bleu']:.4f}\")\n",
    "                print(f\"Number of comparisons: {scores['num_comparisons']}\")\n",
    "            else:\n",
    "                print(\"No valid comparisons found for this model.\")\n",
    "        \n",
    "        # Optionally save results to a file\n",
    "        # results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "        # results_df.to_csv('bleu_scores_results.csv')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize sentence transformer model (using a lightweight model)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_sentence_similarity(reference_text, candidate_text):\n",
    "    \"\"\"Calculate cosine similarity between sentence embeddings.\"\"\"\n",
    "    if pd.isna(reference_text) or pd.isna(candidate_text):\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        # Encode both sentences\n",
    "        embeddings = model.encode([str(reference_text), str(candidate_text)])\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating similarity: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def calculate_model_similarities(df):\n",
    "    \"\"\"Calculate sentence similarities for each model based on specified conditions.\"\"\"\n",
    "    required_columns = ['model_name', 'Dimension', 'firstTaskGoal', 'addTaskGoals', 'Selected_Text']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame is missing required column: {col}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model in df['model_name'].unique():\n",
    "        model_df = df[df['model_name'] == model]\n",
    "        similarities = []\n",
    "        \n",
    "        for _, row in model_df.iterrows():\n",
    "            if row['Dimension'] in ['Uniqueness', 'Appreciation']:\n",
    "                reference_text = row['firstTaskGoal'] if pd.notna(row['firstTaskGoal']) else row['addTaskGoals']\n",
    "            else:\n",
    "                reference_text = row['addFirstRelGoal'] if pd.notna(row['addFirstRelGoal']) else row['addRelGoals']  # Skip if Dimension doesn't match\n",
    "\n",
    "                \n",
    "            candidate_text = row['Selected_Text']\n",
    "            similarity = calculate_sentence_similarity(reference_text, candidate_text)\n",
    "            \n",
    "            if not np.isnan(similarity):\n",
    "                similarities.append(similarity)\n",
    "        \n",
    "        results[model] = similarities if similarities else None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_similarities(results, output_file='model_similarities.svg'):\n",
    "    \"\"\"Visualize sentence similarities across models and save as SVG.\"\"\"\n",
    "    # Prepare data for visualization\n",
    "    viz_data = []\n",
    "    for model, scores in results.items():\n",
    "        if scores:\n",
    "            for score in scores:\n",
    "                viz_data.append({'Model': model, 'Similarity': score})\n",
    "    \n",
    "    if not viz_data:\n",
    "        print(\"No valid similarity scores to visualize\")\n",
    "        return\n",
    "    \n",
    "    df_viz = pd.DataFrame(viz_data)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Create boxplot with jittered points\n",
    "    ax = sns.boxplot(x='Model', y='Similarity', data=df_viz, width=0.6, showfliers=False)\n",
    "    sns.stripplot(x='Model', y='Similarity', data=df_viz, \n",
    "                 color='black', alpha=0.3, jitter=True, size=4, ax=ax)\n",
    "    \n",
    "    # Add average markers\n",
    "    averages = df_viz.groupby('Model')['Similarity'].mean()\n",
    "    for i, model in enumerate(averages.index):\n",
    "        ax.plot([i-0.2, i+0.2], [averages[model], averages[model]], \n",
    "               color='red', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Sentence Similarity Distribution by Model', fontsize=14, pad=20)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "    plt.ylim(0, 1)  # Similarity ranges from 0 to 1\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save as SVG\n",
    "    plt.savefig(output_file, format='svg', bbox_inches='tight')\n",
    "    print(f\"Visualization saved as {output_file}\")\n",
    "    plt.close()\n",
    "\n",
    "def read_and_process_file(file_path):\n",
    "    \"\"\"Read and process the input file.\"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xls', '.xlsx')):\n",
    "        df = pd.read_excel(file_path, sheet_name='Final_Selection')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide CSV or Excel file.\")\n",
    "    \n",
    "    required_columns = ['model_name', 'Dimension', 'firstTaskGoal', 'addTaskGoals', 'addFirstRelGoal', 'addRelGoals', 'Selected_Text']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Input file is missing required column: {col}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    file_path = '/projects/humansVsLLMs/data/EMNLP-HUmanEvaluation.xlsx'  # Change to your file path\n",
    "    output_plot = '/projects/humansVsLLMs/plots/model_similarities.svg'\n",
    "    \n",
    "    try:\n",
    "        df = read_and_process_file(file_path)\n",
    "        results = calculate_model_similarities(df)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"Sentence Similarity Summary by Model:\")\n",
    "        print(\"=\" * 50)\n",
    "        for model, scores in results.items():\n",
    "            if scores:\n",
    "                print(f\"\\nModel: {model}\")\n",
    "                print(f\"  Count: {len(scores)}\")\n",
    "                print(f\"  Average: {np.mean(scores):.4f}\")\n",
    "                print(f\"  Min: {np.min(scores):.4f}\")\n",
    "                print(f\"  Max: {np.max(scores):.4f}\")\n",
    "                print(f\"  Std Dev: {np.std(scores):.4f}\")\n",
    "            else:\n",
    "                print(f\"\\nModel: {model} - No valid scores\")\n",
    "        \n",
    "        # Create and save visualization\n",
    "        visualize_similarities(results, output_plot)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import statistics as s\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate ROUGE for a single pair\n",
    "def compute_rouge(model_df):\n",
    "    # Initialize ROUGE scorer\n",
    "    rouge_scores = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    for _, row in model_df.iterrows():\n",
    "        # Determine reference text based on Dimension\n",
    "        if row['Dimension'] in ['Uniqueness', 'Appreciation']:\n",
    "            reference_text = row['firstTaskGoal'] if pd.notna(row['firstTaskGoal']) else row['addTaskGoals']\n",
    "        else:\n",
    "            reference_text = row['addFirstRelGoal'] if pd.notna(row['addFirstRelGoal']) else row['addRelGoals']  # Skip if Dimension doesn't match   \n",
    "        # Get candidate text\n",
    "        candidate_text = row['Selected_Text']\n",
    "        \n",
    "        # Skip if either text is missing\n",
    "        if pd.isna(reference_text) or pd.isna(candidate_text):\n",
    "            continue\n",
    "        try:\n",
    "            scores = scorer.score(reference_text, candidate_text)\n",
    "            rouge_scores.append(scores)\n",
    "        except:\n",
    "            continue\n",
    "    return rouge_scores\n",
    "    \n",
    "def read_and_process_file(file_path):\n",
    "    \"\"\"\n",
    "    Read and process the input file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file (CSV or Excel)\n",
    "        \n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Read file based on extension\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xls', '.xlsx')):\n",
    "        df = pd.read_excel(file_path, sheet_name='Final_Selection')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide CSV or Excel file.\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['model_name', 'Dimension', 'firstTaskGoal', 'addTaskGoals', 'addFirstRelGoal', 'addRelGoals', 'Selected_Text']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Input file is missing required column: {col}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    file_path = '/projects/humansVsLLMs/data/EMNLP-HUmanEvaluation.xlsx'  # Change to your file path\n",
    "    try:\n",
    "    # Read and process the file\n",
    "        df = read_and_process_file(file_path)\n",
    "        # Get unique models\n",
    "        models = df['model_name'].unique()\n",
    "        R1 = []\n",
    "        R2 = []\n",
    "        R3 = []\n",
    "        for model in models:\n",
    "            model_df = df[df['model_name'] == model]\n",
    "            rouge_scores = compute_rouge(model_df)\n",
    "            for i in range(len(rouge_scores)):\n",
    "                r1 = rouge_scores[i]['rouge1'].fmeasure\n",
    "                r2 = rouge_scores[i]['rouge2'].fmeasure\n",
    "                rl = rouge_scores[i]['rougeL'].fmeasure\n",
    "                R1.append(r1)\n",
    "                R2.append(r2)\n",
    "                R3.append(rl)\n",
    "            print(f\"Aggregated ROUGE Scores for model {model}:\")\n",
    "            print(f\"R1: {s.mean(R1)} | R2: {s.mean(R2)} | RL: {s.mean(R3)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/neural-dialogue-metrics/Distinct-N/tree/main/distinct_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__all__ = [\"ngrams\"]\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, n, pad_left=False, pad_right=False,\n",
    "                 left_pad_symbol=None, right_pad_symbol=None):\n",
    "    \"\"\"\n",
    "    Returns a padded sequence of items before ngram extraction.\n",
    "\n",
    "        >>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "        ['<s>', 1, 2, 3, 4, 5, '</s>']\n",
    "        >>> list(pad_sequence([1,2,3,4,5], 2, pad_left=True, left_pad_symbol='<s>'))\n",
    "        ['<s>', 1, 2, 3, 4, 5]\n",
    "        >>> list(pad_sequence([1,2,3,4,5], 2, pad_right=True, right_pad_symbol='</s>'))\n",
    "        [1, 2, 3, 4, 5, '</s>']\n",
    "\n",
    "    :param sequence: the source data to be padded\n",
    "    :type sequence: sequence or iter\n",
    "    :param n: the degree of the ngrams\n",
    "    :type n: int\n",
    "    :param pad_left: whether the ngrams should be left-padded\n",
    "    :type pad_left: bool\n",
    "    :param pad_right: whether the ngrams should be right-padded\n",
    "    :type pad_right: bool\n",
    "    :param left_pad_symbol: the symbol to use for left padding (default is None)\n",
    "    :type left_pad_symbol: any\n",
    "    :param right_pad_symbol: the symbol to use for right padding (default is None)\n",
    "    :type right_pad_symbol: any\n",
    "    :rtype: sequence or iter\n",
    "    \"\"\"\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def ngrams(sequence, n, pad_left=False, pad_right=False,\n",
    "           left_pad_symbol=None, right_pad_symbol=None):\n",
    "    \"\"\"\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    For example:\n",
    "\n",
    "        >>> from nltk.util import ngrams\n",
    "        >>> list(ngrams([1,2,3,4,5], 3))\n",
    "        [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
    "\n",
    "    Wrap with list for a list version of this function.  Set pad_left\n",
    "    or pad_right to true in order to get additional ngrams:\n",
    "\n",
    "        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True))\n",
    "        [(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]\n",
    "        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol='</s>'))\n",
    "        [(1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n",
    "        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol='<s>'))\n",
    "        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n",
    "        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n",
    "\n",
    "\n",
    "    :param sequence: the source data to be converted into ngrams\n",
    "    :type sequence: sequence or iter\n",
    "    :param n: the degree of the ngrams\n",
    "    :type n: int\n",
    "    :param pad_left: whether the ngrams should be left-padded\n",
    "    :type pad_left: bool\n",
    "    :param pad_right: whether the ngrams should be right-padded\n",
    "    :type pad_right: bool\n",
    "    :param left_pad_symbol: the symbol to use for left padding (default is None)\n",
    "    :type left_pad_symbol: any\n",
    "    :param right_pad_symbol: the symbol to use for right padding (default is None)\n",
    "    :type right_pad_symbol: any\n",
    "    :rtype: sequence or iter\n",
    "    \"\"\"\n",
    "    sequence = pad_sequence(sequence, n, pad_left, pad_right,\n",
    "                            left_pad_symbol, right_pad_symbol)\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        history.append(next(sequence))\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        yield tuple(history)\n",
    "        del history[0]\n",
    "\n",
    "__all__ = [\"distinct_n_sentence_level\", \"distinct_n_corpus_level\"]\n",
    "\n",
    "\n",
    "def distinct_n_sentence_level(sentence, n):\n",
    "    \"\"\"\n",
    "    Compute distinct-N for a single sentence.\n",
    "    :param sentence: a list of words.\n",
    "    :param n: int, ngram.\n",
    "    :return: float, the metric value.\n",
    "    \"\"\"\n",
    "    n = 1\n",
    "    if len(sentence) == 0:\n",
    "        return 0.0  # Prevent a zero division\n",
    "    distinct_ngrams = set(ngrams(sentence, n))\n",
    "    return len(distinct_ngrams) / len(sentence)\n",
    "\n",
    "\n",
    "def distinct_n_corpus_level(sentences, n):\n",
    "    \"\"\"\n",
    "    Compute average distinct-N of a list of sentences (the corpus).\n",
    "    :param sentences: a list of sentence.\n",
    "    :param n: int, ngram.\n",
    "    :return: float, the average value.\n",
    "    \"\"\"\n",
    "    return sum(distinct_n_sentence_level(sentence, n) for sentence in sentences) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Human Text from Real-Life Leaders\n",
    "df = pd.read_csv('/projects/humansVsLLMs/data/goals_leader_with_demographics.csv')\n",
    "# Apply the function row-wise\n",
    "sentence_scores = df['Leader_Action_Plans'].apply(lambda row: distinct_n_sentence_level(row, 2))\n",
    "corpus_scores = df['Leader_Action_Plans'].apply(lambda row: distinct_n_sentence_level(row, 2))\n",
    "print(f'distinct_n_sentence_level: {sentence_scores.mean()} | corpus_scores: {corpus_scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-Generated Texts\n",
    "path = '/projects/humansVsLLMs/results/3-shot-generated-responses'\n",
    "models = ['cohere', 'deepseek', 'gemini', 'gpt-4o-mini', 'llama', 'mistral', 'qwen']\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'{path}/{model}_generated_responses.csv')\n",
    "    # Apply the function row-wise\n",
    "    sentence_scores = df['Response'].apply(lambda row: distinct_n_sentence_level(row, 1))\n",
    "    corpus_scores = df['Response'].apply(lambda row: distinct_n_sentence_level(row, 1))\n",
    "    print(f'distinct_n_sentence_level: {sentence_scores.mean()} | corpus_scores: {corpus_scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

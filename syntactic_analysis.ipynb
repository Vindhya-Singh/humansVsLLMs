{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from itertools import chain\n",
    "# from distinct_n import distinct_n_sentence_level  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sentence length\n",
    "def sentence_length(text):\n",
    "    sen_length = len(text.split())\n",
    "    return sen_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For general, overall comparison\n",
    "df = pd.read_csv('/projects/humansVsLLMs/data/data.csv')\n",
    "for text in df['value']:\n",
    "    sentence_len = sentence_length(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gender wise comparison\n",
    "path = '/projects/humansVsLLMs/data'\n",
    "df = pd.read_csv(f\"{path}/data_leaders_with_demographics_semantics.csv\")  # Replace with your file\n",
    "df = df[df['GenderIdentity'] == 'Female']\n",
    "response_texts_uniqueness = df['firstTaskGoal'].dropna().astype(str).to_list()\n",
    "print(len(response_texts_uniqueness))\n",
    "response_texts_belongingness = df['addFirstRelGoal'].dropna().astype(str).to_list()\n",
    "print(len(response_texts_belongingness))\n",
    "response_texts = response_texts_belongingness + response_texts_uniqueness\n",
    "print(len(response_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Parts of Speech (POS) distribution\n",
    "def pos_distribution(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = {pos: 0 for pos in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"DET\", \"ADP\", \"CONJ\", \"NUM\", \"PUNCT\"]}\n",
    "    total_tokens = len(doc)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1\n",
    "    \n",
    "    return {pos: count / total_tokens if total_tokens > 0 else 0 for pos, count in pos_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Words per Conversation Statement\n",
    "def words_per_statement(text):\n",
    "    sentences = list(nlp(text).sents)\n",
    "    return sum(len(sent.text.split()) for sent in sentences) / len(sentences) if sentences else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process dataset\n",
    "def process_text_data(file_path, column: None):\n",
    "    results = []\n",
    "    if column is not None:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.dropna(subset=[column])  # Remove rows where 'Text' column is empty\n",
    "        \n",
    "        results = []\n",
    "        for text in df[column]:\n",
    "            sentence_len = sentence_length(text)\n",
    "            pos_dist = pos_distribution(text)\n",
    "            # lex_div = lexicon_diversity(text)\n",
    "            words_per_stmt = words_per_statement(text)\n",
    "            \n",
    "            results.append({\n",
    "                \"Sentence Length\": sentence_len,\n",
    "                # \"Lexicon Diversity\": lex_div,\n",
    "                \"Words per Statement\": words_per_stmt,\n",
    "                **pos_dist  # Expanding POS distribution dictionary\n",
    "            })\n",
    "        \n",
    "        # Convert results into DataFrame and normalize by sentence length\n",
    "        results_df = pd.DataFrame(results)\n",
    "        # results_df = results_df.div(results_df[\"Sentence Length\"], axis=0)\n",
    "        # results_df.fillna(0, inplace=True)  # Handle division by zero cases\n",
    "    else:\n",
    "        pass # Run the below text for gender-wise data. Pass texts as a list\n",
    "        # results = []\n",
    "        # for text in texts:\n",
    "        #     sentence_len = sentence_length(text)\n",
    "        #     pos_dist = pos_distribution(text)\n",
    "        #     # lex_div = lexicon_diversity(text)\n",
    "        #     words_per_stmt = words_per_statement(text)\n",
    "            \n",
    "        #     results.append({\n",
    "        #         \"Sentence Length\": sentence_len,\n",
    "        #         # \"Lexicon Diversity\": lex_div,\n",
    "        #         \"Words per Statement\": words_per_stmt,\n",
    "        #         **pos_dist  # Expanding POS distribution dictionary\n",
    "        #     })\n",
    "        \n",
    "        # # Convert results into DataFrame and normalize by sentence length\n",
    "        # results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "      input_file = \"/projects/humansVsLLMs/data/data.csv\"  # Replace with actual file path\n",
    "      result_df = process_text_data(input_file, column=None, texts=response_texts)\n",
    "      result_df.to_csv(\"/projects/humansVsLLMs/results/text_analysis_results_new.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-generated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model = 'gemini'\n",
    "path = \"/projects/humansVsLLMs/results/4-Dimensions-0Shot-GeneratedResponses\"\n",
    "# List of CSV file paths\n",
    "csv_files = [f\"{path}/generated_responses_Generated_Prompt_Uniqueness_{model}_zeroShot.csv\",\n",
    "             f\"{path}/generated_responses_Generated_Prompt_Belongingness_{model}_zeroShot.csv\",\n",
    "             f\"{path}/generated_responses_Generated_Prompt_Appreciation_{model}_zeroShot.csv\", \n",
    "             f\"{path}/generated_responses_Generated_Prompt_OrgEfforts_{model}_zeroShot.csv\"]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)  # Read CSV file\n",
    "    dataframes.append(df)   # Append DataFrame to list\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(merged_df.shape)\n",
    "\n",
    "# Save as csv\n",
    "merged_df.to_csv(f'/projects/humansVsLLMs/results/0-shot-generated-responses/{model}_generated_responses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = 'gemini'\n",
    "    input_file = f\"/projects/humansVsLLMs/results/0-shot-generated-responses/{model}_generated_responses.csv\"  # Replace with actual file path\n",
    "    result_df = process_text_data(input_file, column='Response')\n",
    "    result_df.to_csv(f\"/projects/humansVsLLMs/results/syntactic_analysis/0-shot/{model}_text_analysis.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving excel data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/projects/humansVsLLMs/data/Oct22_Jap_goals.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['firstTaskGoal', 'addTaskGoals', 'addFirstRelGoal', 'addRelGoals', 'addRelGoalsLater']\n",
    "df_new = df[cols]\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.reset_index()\n",
    "df_new = df_new.drop(columns=['level_0'])\n",
    "\n",
    "df_newer = df_new.melt(id_vars=['index']).value.dropna() \n",
    "print(df_newer.shape)\n",
    "df_newer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newer.to_csv('/projects/humansVsLLMs/data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

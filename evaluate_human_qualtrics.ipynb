{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'projects/humansVsLLMs/results/qualtrics/surveys'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Raw BEST and WORST input data\n",
    "best_raw = {\n",
    "    \"Accuracy\": [\"Female Leaders (3)\", \"DeepSeek (3)\", \"Gemini (3)\"],\n",
    "    \"Relevance\": [\"Gemini (3)\", \"Qwen (2)\"],\n",
    "    \"Currency\": [\"Gemini (2)\", \"Cohere (2)\", \"Female (1)\", \"Male (1)\"],\n",
    "    \"Comprehensiveness\": [\"Gemini (4)\", \"GPT (3)\"],\n",
    "    \"Agreement\": [\"Gemini (2)\", \"Male (2)\"],\n",
    "    \"Usefulness\": [\"DeepSeek (5)\"],\n",
    "    \"Clarity\": [\"Gemini (4)\", \"Cohere (2)\", \"Male (1)\", \"Female (1)\"],\n",
    "    \"Empathy\": [\"Llama (4)\"],\n",
    "    \"Bias\": [\"Cohere (3)\", \"Male (3)\", \"Llama (2)\"],\n",
    "    \"Fabrication\": [\"Gemini (3)\", \"Mistral (2)\", \"GPT (2)\"],\n",
    "    \"Trust\": [\"GPT (2)\", \"Gemini (2)\", \"Female (1)\", \"Male (1)\"],\n",
    "    \"Satisfaction\": [\"Qwen (4)\", \"Cohere (2)\", \"DeepSeek (2)\"]\n",
    "}\n",
    "\n",
    "worst_raw = {\n",
    "    \"Accuracy\": [\"Female (3)\", \"Male (2)\", \"DeepSeek (4)\"],\n",
    "    \"Relevance\": [\"Female (4)\", \"Male (2)\", \"DeepSeek (2)\"],\n",
    "    \"Currency\": [\"Female (3)\", \"Male (2)\"],\n",
    "    \"Comprehensiveness\": [\"Male (6)\"],\n",
    "    \"Agreement\": [\"Male (4)\"],\n",
    "    \"Usefulness\": [\"Female (3)\", \"Qwen (2)\", \"Cohere (2)\"],\n",
    "    \"Clarity\": [\"Female (3)\", \"Gemini (2)\"],\n",
    "    \"Empathy\": [\"Male (3)\", \"Female (1)\", \"DeepSeek (2)\", \"Cohere (2)\"],\n",
    "    \"Bias\": [\"Male (2)\", \"GPT (2)\", \"DeepSeek (2)\", \"Female (1)\"],\n",
    "    \"Fabrication\": [\"Female (4)\", \"Male (1)\", \"Gemini (2)\", \"Cohere (2)\"],\n",
    "    \"Trust\": [\"Male (3)\", \"Female (2)\", \"Mistral (2)\"],\n",
    "    \"Satisfaction\": [\"Female (4)\", \"Male (2)\", \"Gemini (2)\"]\n",
    "}\n",
    "\n",
    "# Step 2: Normalize all names\n",
    "label_map = {\n",
    "    \"GPT\": \"gpt-4o-mini\",\n",
    "    \"Gemini\": \"Gemini-2.0-Flash\",\n",
    "    \"Cohere\": \"Command-a-03-2025\",\n",
    "    \"Mistral\": \"Mistral-Large\",\n",
    "    \"Llama\": \"Llama-3.3-70b\",\n",
    "    \"DeepSeek\": \"DeepSeek-R1\",\n",
    "    \"Qwen\": \"Qwen-Plus\",\n",
    "    \"Female\": \"Female Leaders\",\n",
    "    \"Female Leaders\": \"Female Leaders\",\n",
    "    \"Male\": \"Male Leaders\"\n",
    "}\n",
    "\n",
    "# Count frequencies\n",
    "def count_mentions(raw_dict):\n",
    "    counter = Counter()\n",
    "    for items in raw_dict.values():\n",
    "        for entry in items:\n",
    "            match = re.match(r\"([A-Za-z\\s\\-]+)(?:\\s*\\((\\d+)\\))?\", entry.strip())\n",
    "            if match:\n",
    "                label = match.group(1).strip()\n",
    "                label = label_map.get(label, label)\n",
    "                count = int(match.group(2)) if match.group(2) else 1\n",
    "                counter[label] += count\n",
    "    return counter\n",
    "\n",
    "best_counter = count_mentions(best_raw)\n",
    "worst_counter = count_mentions(worst_raw)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "all_labels = set(best_counter) | set(worst_counter)\n",
    "data = []\n",
    "for label in sorted(all_labels):\n",
    "    best = best_counter.get(label, 0)\n",
    "    worst = worst_counter.get(label, 0)\n",
    "    total = best + worst if (best + worst) > 0 else 1\n",
    "    data.append({\n",
    "        \"Source\": label,\n",
    "        \"Best\": best,\n",
    "        \"Worst\": worst,\n",
    "        \"Best (%)\": round(100 * best / total, 1),\n",
    "        \"Worst (%)\": round(100 * worst / total, 1)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(by=\"Best (%)\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "bar_width = 0.4\n",
    "index = range(len(df))\n",
    "\n",
    "plt.bar([i - bar_width/2 for i in index], df[\"Best\"], width=bar_width, label=\"Best\", color=\"green\")\n",
    "plt.bar([i + bar_width/2 for i in index], df[\"Worst\"], width=bar_width, label=\"Worst\", color=\"red\")\n",
    "\n",
    "plt.xticks(index, df[\"Source\"], rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Best vs Worst Evaluations by Source\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Dimension': [\n",
    "        'Accuracy', 'Relevance', 'Currency', 'Comprehensiveness', 'Agreement', 'Usefulness', \n",
    "        'Clarity', 'Empathy', 'Bias', 'Fabrication', 'Trust', 'Satisfaction'\n",
    "    ],\n",
    "    'Best': [\n",
    "        'Female Leaders, DeepSeek, Gemini (3)', \n",
    "        'Gemini (3), Qwen (2)', \n",
    "        'Gemini (2), Cohere (2), Female Leaders (1), Male Leaders (1)', \n",
    "        'Gemini (4), GPT (3)', \n",
    "        'Gemini (2), Male Leaders (2)', \n",
    "        'DeepSeek (5)', \n",
    "        'Gemini (4), Cohere (2), Male Leaders (1), Female Leaders (1)', \n",
    "        'Llama (4)', \n",
    "        'Cohere (3), Male Leaders (3), Llama (2)', \n",
    "        'Gemini (3), Mistral (2), GPT (2)', \n",
    "        'GPT (2), Gemini (2), Female Leaders (1), Male Leaders (1)', \n",
    "        'Qwen (4), Cohere (2), DeepSeek (2)'\n",
    "    ],\n",
    "    'Worst': [\n",
    "        'Female Leaders (3), Male Leaders (2), DeepSeek (4)', \n",
    "        'Female Leaders (4), Male Leaders (2), DeepSeek (4)', \n",
    "        'Female Leaders (3), Male Leaders (2)', \n",
    "        'Male Leaders (6)', \n",
    "        'Male Leaders (4)', \n",
    "        'Female Leaders (3), Qwen (2), Cohere (2)', \n",
    "        'Female Leaders (3), Gemini (2)', \n",
    "        'Male Leaders (3), Female Leaders (1), DeepSeek (2), Cohere (2)', \n",
    "        'Male Leaders (2), GPT (2), DeepSeek (2), Female Leaders (1)', \n",
    "        'Female Leaders (4), Male Leaders (1), Gemini (2), Cohere (2)', \n",
    "        'Male Leaders (3), Female Leaders (2), Mistral (2)', \n",
    "        'Female Leaders (4), Male Leaders (2), Gemini (2)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Helper function to extract model counts\n",
    "def extract_counts(text):\n",
    "    counts = {}\n",
    "    for entry in text.split(','):\n",
    "        entry = entry.strip()\n",
    "        match = re.match(r\"([A-Za-z\\s\\-]+)(?:\\s*\\((\\d+)\\))?\", entry)\n",
    "        if match:\n",
    "            model = match.group(1).strip()\n",
    "            count = int(match.group(2)) if match.group(2) else 1\n",
    "            counts[model] = count\n",
    "    return counts\n",
    "\n",
    "# Flatten data into long format\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    best = extract_counts(row['Best'])\n",
    "    worst = extract_counts(row['Worst'])\n",
    "    dimension = row['Dimension']\n",
    "    \n",
    "    for model, count in best.items():\n",
    "        records.append({'Dimension': dimension, 'Model': model, 'Score Type': 'Best', 'Count': count})\n",
    "    for model, count in worst.items():\n",
    "        records.append({'Dimension': dimension, 'Model': model, 'Score Type': 'Worst', 'Count': count})\n",
    "\n",
    "plot_df = pd.DataFrame(records)\n",
    "# plot_df['Model']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=plot_df, x='Count', y='Dimension', hue='Score Type', orient='h', errorbar=None)\n",
    "# sns.catplot(\n",
    "#     data=plot_df,\n",
    "#     x=\"Count\", y=\"Model\", hue=\"Dimension\",\n",
    "#     kind=\"swarm\", col=\"Score Type\"\n",
    "# )\n",
    "plt.title('Best-Worst Scaling Results')\n",
    "plt.xlabel('Number of Votes')\n",
    "plt.ylabel('Dimension')\n",
    "plt.legend(title='Score Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.to_csv('/humansVsLLMs/plots/qualtrics_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = {\n",
    "    'Model': ['DeepSeek-R1', 'Gemini-2.0-Flash', 'Llama3.3-70b', 'Qwen-Plus', 'GPT-4o mini', 'Mistral-Large', 'Command-a-03-2025'],\n",
    "    'Best Wins': [15, 19, 11, 10, 12, 6, 6],\n",
    "    'Worst Losses': [10, 16, 9, 7, 14, 8, 14],\n",
    "    'Net Score': [5, 3, 2, 3, -2, -2, -8],\n",
    "    'Strongest Positive Dimensions': [\n",
    "        'Usefulness, Trust, Accuracy',\n",
    "        'Clarity, Relevance, Currency, Comprehensiveness',\n",
    "        'Empathy, Satisfaction',\n",
    "        'Satisfaction, Relevance',\n",
    "        'Currency, Usefulness',\n",
    "        'Accuracy',\n",
    "        'Empathy, Coherence'\n",
    "    ],\n",
    "    'Biggest Weaknesses': [\n",
    "        'Bias, Accuracy',\n",
    "        'Fabrication, Bias',\n",
    "        'Fabrication, Clarity',\n",
    "        'Trust, Usefulness',\n",
    "        'Fabrication, Bias',\n",
    "        'Bias, Fabrication',\n",
    "        'Bias, Fabrication, Usefulness'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Melt for grouped barplot\n",
    "df_melt = df.melt(id_vars='Model', value_vars=['Best Wins', 'Worst Losses', 'Net Score'], \n",
    "                  var_name='Metric', value_name='Count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.barplot(data=df_melt, x='Model', y='Count', hue='Metric', palette='Set2')\n",
    "plt.title('Best-Worst Scaling Results')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add annotations for strongest positives and weaknesses\n",
    "for i, row in df.iterrows():\n",
    "    plt.text(i - 0.3, max(df_melt['Count']) + 1, f\"+ {row['Strongest Positive Dimensions']}\", \n",
    "             fontsize=18, color='green', rotation=90)\n",
    "    plt.text(i + 0.1, max(df_melt['Count']) + 1, f\"- {row['Biggest Weaknesses']}\", \n",
    "             fontsize=18, color='red', rotation=90)\n",
    "\n",
    "plt.ylim(0, max(df_melt['Count']) + 10)\n",
    "plt.ylabel('Percentage Share')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = {\n",
    "    'Dimension': ['Accuracy', 'Relevance', 'Currency', 'Comprehensiveness', \n",
    "                 'Coherence', 'Usefulness', 'Clarity', 'Empathy', 'Bias', \n",
    "                 'Fabrication', 'Trust', 'Satisfaction'],\n",
    "    'Best': [\n",
    "        {'Female Leaders': 30, 'DeepSeek-R1': 30, 'Gemini-2.0-Flash': 30},\n",
    "        {'Gemini-2.0-Flash': 30, 'Qwen-Plus': 20},\n",
    "        {'Gemini-2.0-Flash': 20, 'Command-a-03-2025': 20, 'Female Leaders': 10, 'Male Leaders': 10},\n",
    "        {'Gemini-2.0-Flash': 40, 'GPT-4o-mini': 30},\n",
    "        {'Gemini-2.0-Flash': 20, 'Male Leaders': 20},\n",
    "        {'DeepSeek-R1': 50},\n",
    "        {'Gemini-2.0-Flash': 40, 'Command-a-03-2025': 20, 'Male Leaders': 10, 'Female Leaders': 10},\n",
    "        {'Llama-3.3-70b': 40},\n",
    "        {'Command-a-03-2025': 30, 'Male Leaders': 30, 'Llama-3.3-70b': 20},\n",
    "        {'Gemini-2.0-Flash': 30, 'Mistral-Large': 20, 'GPT-4o-mini': 20},\n",
    "        {'GPT-4o-mini': 20, 'Gemini-2.0-Flash': 20, 'Female Leaders': 10, 'Male Leaders': 10},\n",
    "        {'Qwen-Plus': 40, 'Command-a-03-2025': 20, 'DeepSeek-R1': 20}\n",
    "    ],\n",
    "    'Worst': [\n",
    "        {'Female Leaders': 30, 'Male Leaders': 20, 'DeepSeek-R1': 40},\n",
    "        {'Female Leaders': 40, 'Male Leaders': 20, 'DeepSeek-R1': 40},\n",
    "        {'Female Leaders': 30, 'Male Leaders': 20},\n",
    "        {'Male Leaders': 60},\n",
    "        {'Male Leaders': 40},\n",
    "        {'Female Leaders': 30, 'Qwen-Plus': 20, 'Command-a-03-2025': 20},\n",
    "        {'Female Leaders': 30, 'Gemini-2.0-Flash': 20},\n",
    "        {'Male Leaders': 30, 'Female Leaders': 10, 'DeepSeek-R1': 20, 'Command-a-03-2025': 20},\n",
    "        {'Male Leaders': 20, 'GPT-4o-mini': 20, 'DeepSeek-R1': 20, 'Female Leaders': 10},\n",
    "        {'Female Leaders': 40, 'Male Leaders': 10, 'Gemini-2.0-Flash': 20, 'Command-a-03-2025': 20},\n",
    "        {'Male Leaders': 30, 'Female Leaders': 20, 'Mistral-Large': 20},\n",
    "        {'Female Leaders': 40, 'Male Leaders': 20, 'Gemini-2.0-Flash': 20}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract all unique categories\n",
    "all_categories = set()\n",
    "for lst in df['Best']:\n",
    "    all_categories.update(lst.keys())\n",
    "for lst in df['Worst']:\n",
    "    all_categories.update(lst.keys())\n",
    "all_categories = sorted(all_categories)\n",
    "\n",
    "# Create a colormap for the categories\n",
    "colors = plt.cm.tab20.colors\n",
    "cmap = {cat: colors[i % len(colors)] for i, cat in enumerate(all_categories)}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(df))\n",
    "\n",
    "# Plot Best ratings\n",
    "bottom_best = np.zeros(len(df))\n",
    "for category in all_categories:\n",
    "    values = []\n",
    "    for lst in df['Best']:\n",
    "        values.append(lst.get(category, 0))\n",
    "    if sum(values) > 0:  # Only plot if there are values\n",
    "        plt.bar(index - bar_width/2, values, bar_width, \n",
    "                bottom=bottom_best, label=category, color=cmap[category])\n",
    "        bottom_best += np.array(values)\n",
    "\n",
    "# Plot Worst ratings\n",
    "bottom_worst = np.zeros(len(df))\n",
    "for category in all_categories:\n",
    "    values = []\n",
    "    for lst in df['Worst']:\n",
    "        values.append(lst.get(category, 0))\n",
    "    if sum(values) > 0:  # Only plot if there are values\n",
    "        plt.bar(index + bar_width/2, values, bar_width, \n",
    "                bottom=bottom_worst, color=cmap[category])\n",
    "        bottom_worst += np.array(values)\n",
    "\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Ratings Percentage')\n",
    "plt.title('Best and Worst Ratings by Dimension')\n",
    "plt.xticks(index, df['Dimension'], rotation=45, ha='right')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "# # Add labels for total counts\n",
    "# for i in range(len(df)):\n",
    "#     total_best = sum(df.loc[i, 'Best'].values())\n",
    "#     total_worst = sum(df.loc[i, 'Worst'].values())\n",
    "#     plt.text(i - bar_width/2, bottom_best[i] + 0.5, str(total_best), ha='center')\n",
    "#     plt.text(i + bar_width/2, bottom_worst[i] + 0.5, str(total_worst), ha='center')\n",
    "\n",
    "plt.show()\n",
    "# Create a balanced score DataFrame\n",
    "score_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    dimension = row['Dimension']\n",
    "    best = row['Best']\n",
    "    worst = row['Worst']\n",
    "    \n",
    "    for category in all_categories:\n",
    "        best_score = best.get(category, 0)\n",
    "        worst_score = worst.get(category, 0)\n",
    "        net_score = best_score - worst_score\n",
    "        if best_score != 0 or worst_score != 0:\n",
    "            score_data.append([dimension, category, net_score])\n",
    "\n",
    "score_df = pd.DataFrame(score_data, columns=['Dimension', 'Category', 'Net Score'])\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot_df = score_df.pivot(index='Dimension', columns='Category', values='Net Score')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pivot_df.fillna(0), cmap='RdYlGn', aspect='auto')\n",
    "plt.colorbar(label='Net Score (Worst - Best)')\n",
    "plt.xticks(range(len(pivot_df.columns)), pivot_df.columns, rotation=45, ha='right')\n",
    "plt.yticks(range(len(pivot_df.index)), pivot_df.index)\n",
    "plt.title('Net Ratings by Dimension and Data Source')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

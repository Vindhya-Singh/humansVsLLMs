{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Action Plans of leaders with demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'mistral'\n",
    "df = pd.read_csv(f'/projects/humansVsLLMs/results/{model}_generated_responses.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Response'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_life_leaders_demographics_df = pd.read_excel('/projects/humansVsLLMs/data/Inclusion Chatbot (LeaderDemographics)_February 20, 2025_06.43.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1693, 13)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_excel('/projects/humansVsLLMs/data/Oct22_Jap_goals.xlsx')\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 13)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_life_leaders_df = all_data[all_data['uniqueId'].isin(list_of_ids)] # filter participants with socio-demographic data present\n",
    "real_life_leaders_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_life_leaders_df.to_csv('/projects/humansVsLLMs/data/data_leaders_with_demographics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['firstTaskGoal',\n",
    "       'addTaskGoals', 'addFirstRelGoal', 'addRelGoals', 'addRelGoalsLater']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = real_life_leaders_df[selected_columns]\n",
    "df_selected.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN and numeric-only values in each cell\n",
    "def clean_cell(cell):\n",
    "    if pd.isna(cell):\n",
    "        return np.nan\n",
    "    if isinstance(cell, (int, float)) or str(cell).strip().replace('.', '', 1).isdigit():\n",
    "        return np.nan\n",
    "    return str(cell).strip()\n",
    "\n",
    "df_cleaned = df_selected.applymap(clean_cell)\n",
    "\n",
    "# Combine non-NaN values from the selected columns into a single column \n",
    "final_series = df_cleaned.stack().reset_index(drop=True)\n",
    "final_series = final_series.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_series.to_frame(name='Leader_Action_Plans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "final_series.to_frame(name='Leader_Action_Plans').to_csv(\"/projects/humansVsLLMs/data/goals_leader_with_demographics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep for Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/projects/humansVsLLMs/data/data_leaders_with_demographics.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the columns with default values\n",
    "df['Inclusion_Dimension_1'] = None\n",
    "df['Inclusion_Dimension_2'] = None\n",
    "\n",
    "# Loop through unique IDs\n",
    "for uid in df['uniqueId'].unique():\n",
    "    temp_df = df[df['uniqueId'] == uid]\n",
    "\n",
    "    for idx, row in temp_df.iterrows():\n",
    "        # Logic for Inclusion_Dimension_1\n",
    "        if row['stageName'] == 'Goal-Setting' and pd.notna(row['firstTaskGoal']):\n",
    "            df.at[idx, 'Inclusion_Dimension_1'] = 'Uniqueness'\n",
    "        elif row['stageName'] == 'Goal-Setting-Belongingness' and pd.notna(row['firstTaskGoal']):\n",
    "            df.at[idx, 'Inclusion_Dimension_1'] = 'Belongingness'\n",
    "\n",
    "        # Logic for Inclusion_Dimension_2\n",
    "        if row['stageName'] == 'Goal-Setting' and pd.notna(row['addFirstRelGoal']):\n",
    "                df.at[idx, 'Inclusion_Dimension_2'] = 'Appreciation'\n",
    "        elif row['stageName'] == 'Goal-Setting-Belongingness' and pd.notna(row['addFirstRelGoal']):\n",
    "                df.at[idx, 'Inclusion_Dimension_2'] = 'OrgEfforts'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both uniqueId column and ordered_ids list are the same type\n",
    "ordered_ids = list_of_ids\n",
    "df['uniqueId'] = df['uniqueId'].astype(int)\n",
    "ordered_ids = [int(uid) for uid in ordered_ids]\n",
    "\n",
    "# Create a mapping from uniqueId to its order index\n",
    "id_order = {uid: index for index, uid in enumerate(ordered_ids)}\n",
    "\n",
    "# Map the sort index to a new column\n",
    "df['sort_key'] = df['uniqueId'].map(id_order)\n",
    "\n",
    "# Drop rows with NaN sort_key (i.e., uniqueIds not in ordered_ids), if any\n",
    "df = df.dropna(subset=['sort_key'])\n",
    "\n",
    "# Sort in ascending order of sort_key and clean up\n",
    "df_sorted_new = df.sort_values(by='sort_key', ascending=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in ascending order of sort_key and clean up\n",
    "df_sorted_new = df.sort_values(by='sort_key', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_value = {idx: val for idx, val in enumerate(p_ids)}\n",
    "\n",
    "# Convert sort_key to int for indexing\n",
    "df_sorted_new['sort_key'] = df_sorted_new['sort_key'].astype(int)\n",
    "\n",
    "# Add new column based on sort_key index\n",
    "df_sorted_new['PID'] = df_sorted_new['sort_key'].map(index_to_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(df_sorted_new, real_life_leaders_demographics_df, how='left', on=['PID'])\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('/projects/humansVsLLMs/data/data_leaders_with_demographics_semantics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep for human evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/projects/humansVsLLMs/results/3-shot-generated-responses'\n",
    "models = ['cohere', 'deepseek', 'gemini', 'gpt-4o-mini', 'llama', 'mistral', 'qwen']\n",
    "final_df = pd.DataFrame()\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'{file_path}/{model}_generated_responses.csv')\n",
    "    df['Length'] = df['Response'].map(lambda a: len(str(a)))\n",
    "    df['model_name'] = model\n",
    "    new_df = df[df['Length'] < 1500]\n",
    "    final_df = pd.concat([final_df, new_df], ignore_index=True)\n",
    "    print(f'Shape of original {model}_df: {df.shape} and the Shape of new df: {new_df.shape}')\n",
    "print(f'Shape of Final df: {final_df.shape}')\n",
    "final_df.to_csv('/projects/humansVsLLMs/data/human_evaluators_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/projects/humansVsLLMs/results/3-shot-generated-responses'\n",
    "models = ['cohere', 'deepseek', 'gemini', 'gpt-4o-mini', 'llama', 'mistral', 'qwen']\n",
    "df_llm_texts = pd.DataFrame()\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'{file_path}/{model}_generated_responses.csv')\n",
    "    df['Length'] = df['Response'].map(lambda a: len(str(a)))\n",
    "    df['model_name'] = model\n",
    "    df_llm_texts = pd.concat([df_llm_texts, df], ignore_index=True)\n",
    "    print(f'Shape of original {model}_df: {df.shape}')\n",
    "print(f'Shape of Final df: {df_llm_texts.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/projects/humansVsLLMs/results/3-shot-generated-responses'\n",
    "models = ['cohere', 'deepseek', 'gemini', 'gpt-4o-mini', 'llama', 'mistral', 'qwen']\n",
    "final_df = pd.DataFrame()\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'{file_path}/{model}_generated_responses.csv')\n",
    "    df['Length'] = df['Response'].map(lambda a: len(str(a)))\n",
    "    df['model_name'] = model\n",
    "    new_df = df[df['Length'] < 1500]\n",
    "    final_df = pd.concat([final_df, new_df], ignore_index=True)\n",
    "    print(f'Shape of original {model}_df: {df.shape} and the Shape of new df: {new_df.shape}')\n",
    "print(f'Shape of Final df: {final_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files into DataFrames\n",
    "df_real_life_leader = pd.read_csv('/projects/humansVsLLMs/data/data_leaders_with_demographics_semantics.csv')  # Replace with your file path\n",
    "# df_llm_texts = pd.read_csv('/projects/humansVsLLMs/data/human_evaluators_sample.csv')               # Replace with your file path\n",
    "\n",
    "# Extract first 200 characters from the 4 target columns in df_real_life_leader\n",
    "target_columns = [\n",
    "    'Generated_Prompt_Uniqueness',\n",
    "    'Generated_Prompt_Belongingness',\n",
    "    'Generated_Prompt_Appreciation',\n",
    "    'Generated_Prompt_OrgEfforts'\n",
    "]\n",
    "\n",
    "# Get all unique first 200 characters from the 4 columns\n",
    "matching_200_chars = set()\n",
    "for col in target_columns:\n",
    "    # Drop NaN values and take first 200 chars of each non-NaN entry\n",
    "    prompts_200_chars = df_real_life_leader[col].dropna().apply(lambda x: str(x)[:150])\n",
    "    matching_200_chars.update(prompts_200_chars.unique())\n",
    "\n",
    "# Filter df_llm_texts to retain rows where first 200 chars of 'Prompt' match\n",
    "df_llm_texts['Prompt_200_chars'] = df_llm_texts['Prompt'].apply(lambda x: str(x)[:150])\n",
    "df_llm_texts_filtered = df_llm_texts[df_llm_texts['Prompt_200_chars'].isin(matching_200_chars)].copy()\n",
    "\n",
    "# Reorder df_llm_texts_filtered to match df_real_life_leader's prompt order\n",
    "# Create a mapping of first 200 chars to their first occurrence index in df_real_life_leader\n",
    "prompt_order_mapping = {}\n",
    "for idx, row in df_real_life_leader.iterrows():\n",
    "    for col in target_columns:\n",
    "        prompt = str(row[col])[:150] if pd.notna(row[col]) else None\n",
    "        if prompt and prompt not in prompt_order_mapping:\n",
    "            prompt_order_mapping[prompt] = idx  # Track first occurrence\n",
    "\n",
    "# Assign 'order' based on the mapping and sort\n",
    "df_llm_texts_filtered['order'] = df_llm_texts_filtered['Prompt_200_chars'].map(prompt_order_mapping)\n",
    "df_llm_texts_filtered.sort_values('order', inplace=True)\n",
    "df_llm_texts_filtered.drop(['order', 'Prompt_200_chars'], axis=1, inplace=True)  # Cleanup\n",
    "\n",
    "# Reset index\n",
    "df_llm_texts_filtered.reset_index(drop=True, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

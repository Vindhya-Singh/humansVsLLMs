{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functions\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    return sentence_bleu([reference.split()], hypothesis.split())\n",
    "\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)[0]\n",
    "    return scores['rouge-l']['f']  # Using ROUGE-L F1-score\n",
    "\n",
    "def calculate_meteor(reference, hypothesis):\n",
    "    return meteor_score([reference.split()], hypothesis.split())\n",
    "\n",
    "def calculate_bert_similarity(reference, hypothesis):\n",
    "    ref_embedding = model.encode(reference, convert_to_tensor=True)\n",
    "    hyp_embedding = model.encode(hypothesis, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(ref_embedding, hyp_embedding).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame ('Reference' and 'Generated')\n",
    "model = 'gpt-4o-mini'\n",
    "reference_df = pd.read_csv('/projects/humansVsLLMs/data/goals_leader_with_demographics.csv')  # Replace with actual file path\n",
    "generated_df = pd.read_csv(f'/projects/humansVsLLMs/results/{model}_responses.csv') # Replace with actual file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store scores\n",
    "bleu_scores, rouge_scores, meteor_scores, bert_scores_precision, bert_scores_recall, bert_scores_f1, bert_scores = [], [], [], [], [], [], []\n",
    "\n",
    "# Iterate through each row and compute scores\n",
    "for ref, gen in zip(reference_df['Leader_Action_Plans'], generated_df['Response']):\n",
    "    bleu_scores.append(calculate_bleu(ref, gen))\n",
    "    rouge_scores.append(calculate_rouge(ref, gen))\n",
    "    meteor_scores.append(calculate_meteor(ref, gen))\n",
    "    P, R, F1 = score(ref, gen, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "    bert_scores_precision.append(P)\n",
    "    bert_scores_recall.append(R)\n",
    "    bert_scores_f1.append(F1)\n",
    "    bert_scores.append(calculate_bert_similarity(ref, gen))\n",
    "\n",
    "# Normalize scores\n",
    "generated_df['BLEU'] = np.array(bleu_scores) / max(bleu_scores)\n",
    "generated_df['ROUGE'] = np.array(rouge_scores) / max(rouge_scores)\n",
    "generated_df['METEOR'] = np.array(meteor_scores) / max(meteor_scores)\n",
    "generated_df['BERT_Score_Precision'] = np.array(bert_scores_precision) / max(bert_scores_precision)\n",
    "generated_df['BERT_Score_Recall'] = np.array(bert_scores_recall) / max(bert_scores_recall)\n",
    "generated_df['BERT_Score_F1'] = np.array(bert_scores_f1) / max(bert_scores_f1)\n",
    "\n",
    "# Save results\n",
    "generated_df.to_csv('output_scores.csv', index=False)\n",
    "print(\"Scores saved to output_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTR, MATTR, Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from textstat import flesch_reading_ease\n",
    "from lexicalrichness import LexicalRichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_candidate(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_readability_lexical(candidate_df, text_col='Response'):\n",
    "    readability = []\n",
    "    ttr_scores = []\n",
    "    mattr_scores = []\n",
    "\n",
    "    for text in candidate_df[text_col]:\n",
    "        readability.append(flesch_reading_ease(text))\n",
    "        lex = LexicalRichness(text)\n",
    "        ttr_scores.append(lex.ttr)\n",
    "        mattr_scores.append(lex.mattr(window_size=3))  \n",
    "\n",
    "    return readability, ttr_scores, mattr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_scores(readability, ttr, mattr, metric_scores=None):\n",
    "    if metric_scores:\n",
    "        df_scores = pd.DataFrame(metric_scores)\n",
    "    else:\n",
    "        df_scores = pd.DataFrame()\n",
    "        df_scores['Readability'] = readability\n",
    "        df_scores['TTR'] = ttr\n",
    "        df_scores['MATTR'] = mattr\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(model_name):\n",
    "    candidate_df = load_candidate(f'/projects/humansVsLLMs/results/0-shot-generated-responses/{model_name}_generated_responses.csv')\n",
    "    print(candidate_df.shape)\n",
    "    readability, ttr, mattr = calculate_readability_lexical(candidate_df)\n",
    "    df_scores = compile_scores(readability, ttr, mattr)\n",
    "    # Save or display results\n",
    "    df_scores.to_csv(f'/projects/humansVsLLMs/results/semantic_analysis/0-shot/{model_name}_evaluation_scores.csv', index=False)\n",
    "    print(df_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(model_name='gemini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability_lexical(response_texts):\n",
    "    readability = []\n",
    "    ttr_scores = []\n",
    "    mattr_scores = []\n",
    "\n",
    "    for text in response_texts:\n",
    "        readability.append(flesch_reading_ease(text))\n",
    "        lex = LexicalRichness(text)\n",
    "        ttr_scores.append(lex.ttr)\n",
    "        mattr_scores.append(lex.mattr(window_size=2))  \n",
    "    return readability, ttr_scores, mattr_scores\n",
    "\n",
    "def compile_scores(readability, ttr, mattr):\n",
    "    df_scores = pd.DataFrame()\n",
    "    df_scores['Readability'] = readability\n",
    "    df_scores['TTR'] = ttr\n",
    "    df_scores['MATTR'] = mattr\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gender wise comparison\n",
    "gender = \"NB\"\n",
    "path = '/projects/humansVsLLMs/data'\n",
    "df = pd.read_csv(f\"{path}/data_leaders_with_demographics_semantics.csv\")  # Replace with your file\n",
    "df = df[~df['GenderIdentity'].isin([\"Female\", \"Male\"])]\n",
    "response_texts_uniqueness = df['firstTaskGoal'].dropna().astype(str).to_list()\n",
    "print(len(response_texts_uniqueness))\n",
    "response_texts_belongingness = df['addFirstRelGoal'].dropna().astype(str).to_list()\n",
    "print(len(response_texts_belongingness))\n",
    "response_texts = response_texts_belongingness + response_texts_uniqueness\n",
    "print(len(response_texts))\n",
    "\n",
    "# metrics = calculate_metrics(reference_df, candidate_df)\n",
    "readability, ttr, mattr = calculate_readability_lexical(response_texts)\n",
    "df_scores = compile_scores(readability, ttr, mattr)\n",
    "\n",
    "# Save or display results\n",
    "df_scores.to_csv(f'/projects/humansVsLLMs/results/semantic_analysis/{gender}_evaluation_scores.csv', index=False)\n",
    "print(df_scores.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
